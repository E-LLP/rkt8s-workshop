Basics with rkt, the container engine by CoreOS

27th June 2016

Sergiusz Urbaniak
rkt Engineer, CoreOS
sur@coreos.com
@_surbaniak

* whoami

Software Engineer at CoreOS

rkt developer

Yes ... I do use the Linux Desktop :-)

* Overview

.image rkt8s.png _ 600

- Learn about rkt
- Use rkt
- Learn about Kubernetes
- Use Kubernetes with rkt

Requirements:

- Vagrant
- Virtualbox

* Setup

Open this presentation at

.link http://goo.gl/kUuiyN

Create this `Vagrantfile`:

  Vagrant.configure("2") do |config|
    config.vm.box = "sur/rktnetes"
    config.vm.provider "virtualbox" do |vb|
      vb.gui = false
      vb.memory = "2048"
      vb.cpus = "2"
    end

    config.vm.network "private_network", type: "dhcp"
    config.vm.synced_folder '.', '/vagrant', disabled: true
  end

Execute:

  vagrant box add virtualbox.box --name sur/rkttnetes
  vagrant up --provider=virtualbox

* rkt - What is it?

rkt (pronounced _rock-it_) is a CLI for running app containers on Linux. rkt is designed to be secure, composable, and standards-based.

- It runs on Linux only (currently).

- It runs on Intel `amd64`.

- Port to `arm64` is underway.

* rkt - a brief history

*December*2014* - _v0.1.0_

- Prototype
- Drive conversation (security, standards) and competition (healthy OSS) in container ecosystem

*February*2016* - _v1.0.0_

- Used inProduction
- API stability guarantees

*June*2016* - _v1.9.0_

- Packaged in Debian, Fedora, Arch, NixOS

* rkt - Philosophy, Idioms

*UX:*"secure-by-default"*

- Verify image signatures by default
- Verify image integrity by default

*Architecture:*Unix*philosophy*

- Well-defined operations
- No central priviledged long-running components
- Separate privileges for different operations, i.e. "fetch"
- Verbose at times

* rkt - Security

- User namespaces (Container `euid` `!=` `host_euid`)

- SELinux contexts (Isolate individual pods)

- VM containment (LKVM)

- TPM measurements (Tamper-proof audit log of what's running)

* rkt - Security (soon)

- Finer-grained Linux capabilities
- Seccomp defaults
- Better SELinux support
- Cgroup2 + cgroup namespaces support
- Qemu (hypervisor) support
- Unpriviledged containers

* rkt - Composability

*External*

- Integrate with existing init system(s)
- Work well with higher level cluster orchestration tools (i.e. Kubernetes)
- Simple process model: rkt command _is_ the container
- Any context applied to rkt (cgroups, etcd.) applies transitively to the pods inside rkt
- Optional gRPC API server

*Internal*

- stage based architecture
- Swappable execution engines running containers

* rkt vs. Docker

.image rkt-vs-docker-process-model.png 500 _

* rkt vs. Docker

.image rkt-vs-docker-fetch.png 500 _

* rkt - Speaking of images

.image image-standards.png

* rkt - Image support

- ACI - yes
- Docker - yes

Caveat: Docker images do not suport signing, start with `--insecure-options=image`

- OCI - WIP

*Workshop*time*

1. Start nginx

  rkt run --insecure-options=image docker://nginx

2. Start an interactive container

  rkt run --insecure-options=image --interactive docker://progrium/busybox

quit with three-times `Ctrl-]`

* rkt vs. ?

.link https://github.com/coreos/rkt/blob/master/Documentation/rkt-vs-other-projects.md

* rkt stages

Execution with rkt is divided into several distinct stages.

*stage0*

The `rkt` binary itself.

- Fetch images
- Generate pod UUIDs, manifest
- Create filesystem for the pod
- Set up further stages
- Unpack stage1 into pod file system
- Unpack app images into stage2 directories

* rkt stages - stage1

*stage1*:

An image providing:

- Initializes container isolation
- Starts and initializes container supervision
- Reads pods and image manifests
- Starts the actual applications

This is swappable, comes in different "flavors":

- _fly_: does nothing really ... just a chroot environment
- _coreos_: uses Linux cgroups, namespaces, leverages systemd
- _kvm_: Uses LKVM supervisor to boot a real virtualized kernel

* rkt stages - coreos stage1

Provides container isolation

stage1-coreos.aci

  host OS
  └─ rkt
    └─ systemd-nspawn
      └─ systemd
        └─ chroot
          └─ user-app1

- *systemd-nspawn*: Implements the initialization of cgroups, namespaces
- *systemd*: Implements supervision of containers

* rkt stages - fly stage1

stage1-fly.aci

  host OS
  └─ rkt
    └─ chroot
      └─ user-app1

- Just a chroot'ed application

but

- Secure image retrieval including signature validation
- Lightweight, isolated package manager

* rkt stages - ??? stage1

Crazy ideas:

- _xhyve_: OSX virtualization allowing rkt to run natively on a Mac
- _runit_: Don't like systemd? Bring your own isolator/supervisor

PRs/Contributions welcome!

* rkt stages - fly stage2

.image you.png _ 400

* rkt - build your own image

*Workshop*time*

.link https://s-urbaniak.github.io/rkt8s-workshop
.link https://github.com/s-urbaniak/inspector

- Build a small Go application
- Build an ACI image
- Sign the image using `gpg`
- Make the image discoverable via github pages

* Let's step back

.image os-procs.png _ 200

In a classical "OS" setup we have:

- A supervisor, aka "init daemon", aka PID1
- Not only one process, but many processes
- Processes work together, either via localhost net, IPC
- Communicate with outside world

* rkt - Pods

.image pod-apps.png _ 300

- Grouping of applications executing in a shared context (network, namespaces, volumes)
- Shared fate
- The _only_ execution primitive: single applications are modelled as singleton pods

* rkt - Pods

*Workshop*time*

1. Create a pod having two apps

* rkt - Networking

The CNI (Container Network Interface)

.image pod-net.png _ 300

- Abstraction layer for network configuration
- Single API to multiple, extensible networks
- Narrow, simple API
- Plugins for third-party implementations

* rkt - Networking - Host Mode

.image host-mode.png _ 300

  rkt run --net=host ...

- Inherit the network namespace of the process that is invoking rkt.
- Pod apps are able to access everything associated with the host’s network interfaces.

*Workshop*time*

1. Start nginx using `--net=host`

* rkt - Networking - Default Mode (CNI ptp)

.image ptp.png _ 300

  rkt run --net ...
  rkt run --net=default ...

.link https://github.com/containernetworking/cni/blob/master/Documentation/ptp.md

- Creates a virtual ethernet pair
- One placed in the pod
- Other one placed on the host

* rkt - Networking - CNI brigde

.image bridge.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/bridge.md

- Creates a virtual ethernet pair
- One placed in the pod
- Other one placed on the host
- Host veth pluggind into a linux bridge

* rkt - Networking - CNI macvlan

.image macvlan.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/macvlan.md

- Functions like a switch
- Pods get different MAC addresses
- Pods share the same physical device

* rkt - Networking - CNI ipvlan

.image ipvlan.png _ 300

.link https://github.com/containernetworking/cni/blob/master/Documentation/ipvlan.md

- Functions like a switch
- Pods share the same MAC address
- Pods get different IPs
- Pods share the same physical device

* rkt - Networking

*Workshop*time*

1. Configure a ptp network, start two pods
2. Configure a bridget network, start two pods

* rkt - Networking - SDN (software defined networking)

.image pod-net-canal.png 300 _

- Communicate with pods across different _hosts_
- Each pod across all hosts gets its own IP
- Virtual overlay network

* Kubernetes - Overview

.image flower.png

.link https://github.com/kubernetes/kubernetes

- Open Source project initiated by Google
- Cluster-level container orchestration

Handles:

- Scheduling/Upgrades
- Failure recovery
- Scaling

We are working very hard to make *rkt* a first class runtime in Kubernetes.

* k8s - Internals

.image k8s-overview.png 350 _

1. Watches created pods, assigns them to nodes
2. Runs controllers (node ctl, replication ctl, endpoint ctl, service account ctl, ...)
3. Watches for pods assigned to its node, runs *rkt*!
4. Manipulates network rules (iptables) for services on a node, does connection forwarding.

* k8s - Let's do it!

*Workshop*time*

1. Start Kubernetes
2. Create a service
3. Launch nginx
4. Launch busybox
5. Exec into busybox
6. Launch the Kubernetes dashboard
7. Open it from the host machine

See the next slides for templates.

* rkt8s - nginx Service

  kind: Service
  apiVersion: v1
  metadata:
    labels:
      app: nginx
    name: nginx
  spec:
    type: NodePort
    ports:
    - port: 80
      targetPort: 80
    selector:
      app: nginx

* rkt8s - nginx Replication Controller

    kind: ReplicationController
    apiVersion: v1
    metadata:
      labels:
        app: nginx
      name: nginx
    spec:
      replicas: 1
      selector:
        app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx
            ports:
            - containerPort: 80
              protocol: TCP

* rkt8s - busybox pod

  apiVersion: v1
  kind: Pod
  metadata:
    name: busybox-sleep
  spec:
    containers:
    - name: busybox
      image: progrium/busybox
      args:
      - sleep
      - "1000000"

Exec into the pod:

  kubectl exec -ti busybox-sleep /bin/sh

* rkt8s - Dashboard

  kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml
